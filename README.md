# ğŸ¤– Local AI Chat Assistant

Une application de chat IA moderne qui fonctionne entiÃ¨rement en local avec Ollama. Aucune connexion internet requise, confidentialitÃ© totale et contrÃ´le complet de vos donnÃ©es.

## âœ¨ FonctionnalitÃ©s

- ğŸš€ **Chat IA Local** - Utilise des modÃ¨les IA qui tournent sur votre ordinateur
- ğŸ”’ **ConfidentialitÃ© Totale** - Vos donnÃ©es restent sur votre appareil
- ğŸ’¬ **Conversations Multiples** - CrÃ©ez et gÃ©rez plusieurs sessions de chat
- ğŸ¯ **SÃ©lection de ModÃ¨les** - Choisissez parmi diffÃ©rents modÃ¨les IA
- ğŸ’¾ **Sauvegarde Automatique** - Toutes vos conversations sont sauvegardÃ©es automatiquement
- ğŸ“± **Interface Responsive** - Fonctionne sur desktop, tablette et mobile
- ğŸŒ™ **Mode Sombre** - Interface adaptÃ©e Ã  vos prÃ©fÃ©rences
- ğŸ“¤ **Export/Import** - Sauvegardez et restaurez vos conversations

## ğŸš€ Installation Rapide

### Option 1: Installation Locale

#### PrÃ©requis

- **Node.js** (version 16 ou supÃ©rieure)
- **Ollama** - [TÃ©lÃ©charger sur ollama.ai](https://ollama.ai)

#### Installation de l'Application

```bash
# Cloner le projet
git clone <repository-url>
cd local-llama-whisper-chat

# Installer les dÃ©pendances
npm install

# DÃ©marrer l'application
npm run dev
```

#### Configuration d'Ollama

```bash
# Installer Ollama (si pas dÃ©jÃ  fait)
# Voir: https://ollama.ai/download

# DÃ©marrer Ollama
ollama serve

# TÃ©lÃ©charger un modÃ¨le (dans un autre terminal)
ollama pull llama2
```

#### AccÃ©der Ã  l'Application

Ouvrez votre navigateur et allez sur `http://localhost:8081`

### Option 2: Installation avec Docker ğŸ³

#### PrÃ©requis Docker

- **Docker** - [TÃ©lÃ©charger Docker Desktop](https://www.docker.com/products/docker-desktop/)
- **Docker Compose** - Inclus avec Docker Desktop

#### DÃ©marrage Rapide avec Docker

```bash
# Cloner le projet
git clone <repository-url>
cd local-llama-whisper-chat

# DÃ©marrer l'application et Ollama avec Docker Compose
docker-compose up -d

# TÃ©lÃ©charger un modÃ¨le dans le conteneur Ollama
docker exec -it local-llama-whisper-chat-ollama-1 ollama pull llama2
```

#### AccÃ©der Ã  l'Application Docker

- **Application Web** : `http://localhost:8081`
- **API Ollama** : `http://localhost:11434`

#### Gestion des Conteneurs

```bash
# Voir les logs en temps rÃ©el
docker-compose logs -f

# ArrÃªter les services
docker-compose down

# RedÃ©marrer les services
docker-compose restart

# Supprimer les conteneurs et volumes
docker-compose down -v
```

#### Configuration Docker AvancÃ©e

Le fichier `docker-compose.yml` configure :
- **Port 8081** : Application web React
- **Port 11434** : API Ollama
- **Volumes persistants** : ModÃ¨les et donnÃ©es Ollama
- **RÃ©seau interne** : Communication entre services

#### TÃ©lÃ©chargement de ModÃ¨les

```bash
# Lister les modÃ¨les disponibles
docker exec -it local-llama-whisper-chat-ollama-1 ollama list

# TÃ©lÃ©charger diffÃ©rents modÃ¨les
docker exec -it local-llama-whisper-chat-ollama-1 ollama pull llama2
docker exec -it local-llama-whisper-chat-ollama-1 ollama pull codellama
docker exec -it local-llama-whisper-chat-ollama-1 ollama pull mistral

# VÃ©rifier l'espace disque utilisÃ©
docker exec -it local-llama-whisper-chat-ollama-1 du -sh /root/.ollama
```

#### Avantages de Docker

âœ… **Installation simplifiÃ©e** - Pas besoin d'installer Node.js ou Ollama localement  
âœ… **Environnement isolÃ©** - Pas de conflits avec d'autres applications  
âœ… **DÃ©ploiement cohÃ©rent** - MÃªme environnement sur tous les systÃ¨mes  
âœ… **Gestion facile** - Start/stop avec une seule commande  
âœ… **PortabilitÃ©** - Fonctionne sur Windows, macOS, Linux  
âœ… **Sauvegarde simple** - Volumes Docker pour persistance des donnÃ©es

## ğŸ“– Guide d'Utilisation

### PremiÃ¨re Utilisation

1. **Lancez l'application** - Un guide d'accueil s'affichera automatiquement
2. **SÃ©lectionnez un modÃ¨le** - Choisissez parmi les modÃ¨les disponibles
3. **Commencez Ã  chatter** - Tapez votre premier message !

### Interface Principale

#### ğŸ¯ SÃ©lecteur de ModÃ¨le
- SituÃ© en haut de l'interface
- Choisissez le modÃ¨le IA que vous souhaitez utiliser
- Changez de modÃ¨le Ã  tout moment

#### ğŸ’¬ Zone de Chat
- Affiche vos conversations
- Messages utilisateur et IA clairement diffÃ©renciÃ©s
- Indicateur de frappe en temps rÃ©el
- DÃ©filement automatique vers les nouveaux messages

#### ğŸ“‹ Sidebar des Sessions
- Liste toutes vos conversations
- CrÃ©ez de nouvelles sessions avec le bouton "+"
- Cliquez sur une session pour la charger
- Supprimez les sessions inutiles

### FonctionnalitÃ©s AvancÃ©es

#### ğŸ”„ Changement de Session
- Cliquez sur une session dans la sidebar
- L'interface se met Ã  jour automatiquement
- Tous les messages de la session s'affichent

#### ğŸ¨ Mode Sombre
- Basculez entre mode clair et sombre
- PrÃ©fÃ©rence sauvegardÃ©e automatiquement
- Interface adaptÃ©e Ã  vos yeux

#### ğŸ’¾ Sauvegarde des DonnÃ©es
- Toutes les conversations sont sauvegardÃ©es localement
- Pas de perte de donnÃ©es lors du rechargement
- DonnÃ©es stockÃ©es dans le navigateur

## ğŸ› ï¸ ModÃ¨les SupportÃ©s

L'application fonctionne avec tous les modÃ¨les Ollama. Voici quelques suggestions :

### ModÃ¨les RecommandÃ©s

```bash
# ModÃ¨le gÃ©nÃ©ral (recommandÃ© pour dÃ©buter)
ollama pull llama2

# ModÃ¨le plus rÃ©cent et performant
ollama pull llama2:13b

# ModÃ¨le spÃ©cialisÃ© code
ollama pull codellama

# ModÃ¨le franÃ§ais
ollama pull mistral
```

### Ajouter un Nouveau ModÃ¨le

1. Ouvrez un terminal
2. ExÃ©cutez : `ollama pull nom-du-modÃ¨le`
3. Rechargez l'application
4. Le nouveau modÃ¨le apparaÃ®tra dans la liste

## ğŸ”§ DÃ©pannage

### Ollama ne rÃ©pond pas
```bash
# VÃ©rifiez qu'Ollama est dÃ©marrÃ©
ollama serve

# Testez avec un modÃ¨le
ollama run llama2 "Hello"
```

### Aucun modÃ¨le disponible
1. VÃ©rifiez qu'Ollama est en cours d'exÃ©cution
2. TÃ©lÃ©chargez au moins un modÃ¨le : `ollama pull llama2`
3. Rechargez l'application

### L'application ne dÃ©marre pas
```bash
# VÃ©rifiez Node.js
node --version

# RÃ©installez les dÃ©pendances
rm -rf node_modules package-lock.json
npm install
```

### ProblÃ¨mes de performance
- Utilisez des modÃ¨les plus petits pour de meilleures performances
- Fermez les autres applications gourmandes en ressources
- VÃ©rifiez que vous avez assez de RAM (8GB minimum recommandÃ©)

### ProblÃ¨mes Docker

#### Docker ne dÃ©marre pas
```bash
# VÃ©rifiez que Docker Desktop est dÃ©marrÃ©
docker --version

# RedÃ©marrez Docker Desktop si nÃ©cessaire
# Windows/macOS : RedÃ©marrez l'application Docker Desktop
# Linux : sudo systemctl restart docker
```

#### Ports dÃ©jÃ  utilisÃ©s
```bash
# VÃ©rifiez les ports utilisÃ©s
netstat -an | grep 8081
netstat -an | grep 11434

# ArrÃªtez les services qui utilisent ces ports
# Ou modifiez les ports dans docker-compose.yml
```

#### ProblÃ¨mes de permissions (Linux)
```bash
# Ajoutez votre utilisateur au groupe docker
sudo usermod -aG docker $USER

# RedÃ©marrez votre session ou exÃ©cutez
newgrp docker
```

#### Conteneur Ollama ne rÃ©pond pas
```bash
# VÃ©rifiez les logs du conteneur
docker-compose logs ollama

# RedÃ©marrez le conteneur Ollama
docker-compose restart ollama

# VÃ©rifiez l'espace disque disponible
docker system df
```

#### ProblÃ¨mes de mÃ©moire
```bash
# VÃ©rifiez l'utilisation mÃ©moire
docker stats

# Augmentez la mÃ©moire allouÃ©e Ã  Docker Desktop
# Windows/macOS : Settings > Resources > Memory
# Linux : Modifiez /etc/docker/daemon.json
```

## ğŸ“± Utilisation Mobile

L'application est entiÃ¨rement responsive :
- **Sidebar rÃ©tractable** sur mobile
- **Interface adaptÃ©e** aux Ã©crans tactiles
- **Navigation intuitive** avec boutons de menu

## ğŸ”’ SÃ©curitÃ© et ConfidentialitÃ©

- âœ… **Aucune donnÃ©e envoyÃ©e** sur internet
- âœ… **ModÃ¨les locaux** sur votre machine
- âœ… **Sauvegarde locale** dans le navigateur
- âœ… **Aucun tracking** ou analytics
- âœ… **Code open source** vÃ©rifiable

## ğŸ¯ Conseils d'Utilisation

### Organisation des Conversations
- CrÃ©ez des sessions sÃ©parÃ©es pour diffÃ©rents sujets
- Utilisez des titres descriptifs pour vos conversations
- Supprimez rÃ©guliÃ¨rement les sessions inutiles

### Optimisation des Performances
- Utilisez des modÃ¨les adaptÃ©s Ã  vos besoins
- Fermez les sessions non utilisÃ©es
- Nettoyez le cache du navigateur si nÃ©cessaire

### Sauvegarde
- Exportez rÃ©guliÃ¨rement vos conversations importantes
- Gardez des sauvegardes de vos sessions favorites
- Utilisez la fonction d'import pour restaurer des conversations

## ğŸ¤ Contribution

Ce projet est open source ! N'hÃ©sitez pas Ã  :
- Signaler des bugs
- Proposer des amÃ©liorations
- Contribuer au code
- AmÃ©liorer la documentation

## ğŸ“„ Licence

Ce projet est sous licence MIT. Voir le fichier LICENSE pour plus de dÃ©tails.

## ğŸ™ Remerciements

- [Ollama](https://ollama.ai) - Pour l'infrastructure IA locale
- [React](https://reactjs.org) - Framework frontend
- [Tailwind CSS](https://tailwindcss.com) - Framework CSS
- [Shadcn/ui](https://ui.shadcn.com) - Composants UI

---

**Profitez de votre assistant IA personnel et privÃ© ! ğŸš€**
